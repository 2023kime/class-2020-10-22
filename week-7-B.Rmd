---
title: "Week 7, Day 2"
output: html_document
---

```{r setup, include=FALSE}
# We need the PPBDS.data package because it includes the qscores data which we
# will use for this exercise. rstanarm is the package we use for constructing
# Bayesian models. See The Primer for examples on its use. It is probably the
# most popular model in R for doing so, although brms is also widely used.

knitr::opts_chunk$set(echo = FALSE)
library(PPBDS.data)
library(rstanarm)
library(tidyverse)
```

We have now learned two techniques for constructing a posterior probability distribution: building the $p(models, data)$ joint distribution by hand and using the bootstrap. Both are a bother, although the bootstrap is much easier and more flexible. Today, we will practice using `rstanarm::stan_glm()` for the same purpose.

The parameter $H$ is still the average number of hours of work reported by students per course. 

## Scene 1

**Prompt:** Create an objected called `fit_obj` which uses `stan_glm()` to estimate a model which explains hours for courses. It still has two parameters: $H$ and $\sigma$. $H$ is the average hours for courses in the population. $\sigma$ is the variability (around the average) in reported hours in the population. Print the model out and write some bullet points which explain the meaning of each parameter you have just estimated.

Review the Cardinal Virtues which serve as our guide for data science. Under Justice, is this model predictive or causal? What would the Preceptor Table look like? Write down the mathematical model we are using.

```{r}

qscores
fit_obj <- stan_glm(hours ~ 1, 
                    family = gaussian, 
                    data = qscores,
                    refresh = 0)

# The median value for the average number of hours (H variable) is the intercept
# at 6.2 hours. 
# The average standard deviation of hours values from the intercept mu value is 
# 3.5.

# The model is hard to use as either predictive or causal, since it only uses
# one variable. It is rather explanatory, because it shows a pattern in the data.
# The preceptor table looks 
# The mathematical bayesian model is y1 = mu + sigma

```

## Scene 2

**Prompt:** Create a plot of the posterior probability distribution for $H$. Interpret the plot. 

```{r}

fit_obj %>% 
  as_tibble() %>% 
  rename(mu = `(Intercept)`) %>% 
  ggplot(aes(x = mu)) +
  geom_histogram(aes(y = after_stat(count/sum(count))), 
                 binwidth = 0.01, 
                 color = "white") +
  labs(title = "Posterior Probability Distribution",
       subtitle = "Average time spent on class",
       x = "Time in Hours",
       y = "Probability") +
  scale_y_continuous(labels=scales::percent_format())

# The average time spent in hours seems to be grouped around 6.2 hours per class.
  
```


## Scene 3

**Prompt:** Use your model to answer the following questions: 

What do the rows and columns mean in the matrix returned by `posterior_predict()` mean?

Define D as the number of hours difference between the workload of two randomly selected courses. What is the 90% confidence interval within which the difference should fall?  

What is your posterior probability distribution for D? 

```{r}

q3 <- posterior_predict(fit_obj)
course_diff <- tibble(c1 = q3[,1], c2 = q3[,2]) %>%
  mutate(D = c1 - c2)
  quantile(course_diff$D, probs = c(0.05, 0.95))

ggplot(course_diff, aes(course_diff$D)) +
  geom_histogram(aes(y = after_stat(count/sum(count))), 
                 binwidth = 1,
                 color = "white") +
labs(title = "Posterior Probability Distribution",
       subtitle = "Average difference in time spent on class",
       x = "Difference in hours",
       y = "Probability") +
  scale_y_continuous(labels=scales::percent_format()) 

```
